\documentclass{sig-alternate}

\usepackage{alltt}
\usepackage{enumerate}
\usepackage[ruled,vlined]{algorithm2e}

\renewcommand{\CommentSty}[1]{\textnormal{#1}}
\DontPrintSemicolon
\SetKwComment{tcp}{$\triangleright$ }{}
\SetVlineSkip{0cm}
\SetAlgoSkip{}

\let\oldemptyset\emptyset
\let\emptyset\varnothing

\begin{document}

\conferenceinfo{WOODSTOCK}{'97 El Paso, Texas USA}
\title{Adaptive Disk Storage for Interaction Graphs}
\numberofauthors{1} %  in this sample file, there are a *total*

\author{}

\date{}

\maketitle
\begin{abstract}
TODO
\end{abstract}


\section{Introduction}
An \emph{interaction graph} is an append-only graph, where new edges and
vertices are added as time progresses. 
%

% 
Typical interaction graphs have data associated with their vertices and edges.
Most algorithms access this data as they traverse the graph. However, not all
of the data is accessed by all the queries. Typically, there are correlations
among the attributes accessed by different queries, such as Q1 and Q5
accessing attributes a and b, and Q2, Q3, Q4 accessing attributes c and d, and
so on.
%

In an interaction graph database organized as blocks of temporal neighborlists
(as in my earlier paper), it is important to store the edge and vertex
properties locally. For  instance, if the edge properties are kept away in a
relational table, there will be almost no benefit to the locality
optimizations performed for block organization (discussed in \cite{gedik14}), as we would have to go back and forth between the disk blocks to
access the edge attributes. 
%

On the other hand, putting all the edge attributes into the disk blocks
containing the graph structure is expected to cause significant overhead when
only a few of these attributes are read. This is somewhat similar to the 
problem with row-oriented databases, where the entire row needs to be accessed
from the disk despite the query needing only a small fraction of it.
Unfortunately, there is no clear correspondence to a column-oriented database
layout for the graph databases.
%

Recall that interaction graphs are temporal. As such, the co-access
correlations for the attributes can be different for different temporal
regions. It might also  be unknown at the insertion time and may be discovered
later depending on the workload. This points to the need for adaptively
optimizing the layout (somewhat similar to the H2O paper).
%

I imagine a solution to this problem, which I name the 'rail layout'. The idea
is to start with large blocks that contain the entire data. As we learn about
the access properties for different time regions, we might start splitting
such blocks into smaller blocks that run parallel to each other, almost like
having two or more graph databases for certain time regions, each containing a
 different subset of the attributes, but with a link between them in case a
query needs to access both. Some attributes can be replicated as well. 
More details to follow about all this...

\section{Example}

\begin{alltt}\scriptsize
- Work through a concrete example of a database and a 
   couple of queries that would change the layout on disk.
\end{alltt}




\section{Adaptation}

\section{Cost Model}
Let $Q$ be the query workload, where each query $q\in Q$ accesses a set of
attributes $q.A$ and traverses parts of the graph for the time range
$q.T=[q.t_s,q.t_e]$. We denote the set of all attributes as $A$. Given a block
$B$, we denote its time range as $B.T$, which is the union of the time ranges
of its temporal neighborlists. Let $s(a)$ denote the size of an attribute $a$.
We use $c_n(B)$ to denote the number of temporal neighborlists within block
$B$ and $c_e(B)$ to denote the total number of edges in the temporal
neighborlists within the block. We overload the notation for block size and
use $s(B)$ to denote the size of a block $B$. We have: 
\begin{equation}
s(B) = c_e(B) \cdot \left(16 + \sum_{a\in A} s(a)\right) + c_n(B) \cdot 12  
\end{equation}
Here, $16$ corresponds to the cost of storing the edge id and the timestamp,
and $qw$ corresponds to the cost of storing the head vertex ($8$ bytes) plus
the number of entries ($4$ bytes) for a temporal neighborlist. 

Our goal is to create an overlapping partitioning of a block, denoted by
$\mathcal{P}(B)$. We call the partitions \emph{sub-blocks}. We have
$\bigcup_{B'\in \mathcal{P}(B)} B'.A = A$. We aim to find the function
$\mathcal{P}$ that minimizes the query I/O over $B$, while keeping the
relative storage overhead beyond a limit, say $1+\alpha$ times the original.
If we represent the query I/O as $L(\mathcal{P}, B)$ and the relative storage
overhead as $H(\mathcal{P}, B)$, our goal is to find:
\begin{equation}
\mathcal{P} \leftarrow \mbox{argmin}_{\{\mathcal{P}: H(\mathcal{P}(B)) < \alpha\}} L(\mathcal{P},B)
\end{equation}

The storage overhead can be formalized as:
\begin{equation}
H(\mathcal{P}, B) = |\mathcal{P}(B)|\cdot\left(1-\frac{c_e(B)\cdot \sum_{a\in A} s(a)}{s(B)}\right) 
\end{equation}

\subsection{Query I/O}

Let $m$ be a function that maps a query $q$ to the set of sub-blocks that are
accessed to satisfy it for a relevant block $B$ under a given partitioning
$\mathcal{P}$. For the case of non-overlapping attributes, we have:
\begin{equation}
m(\mathcal{P}, B, q) = \{B': B'\in \mathcal{P}(B) \wedge q.A \cap B'.A \ne \emptyset\}  
\end{equation}

For the case of overlapping attributes, we use a simple heuristic to define the
set of sub-blocks to be used for answering the query.
Algorithm~\ref{alg:greedyM} captures it. The idea is to select sub-blocks that
bring the highest relative marginal gain, which is defined as the size of
attribute data that contributes to the query result, relative to the sub-block
size. While computing the marginal gain, attributes that are covered by
sub-blocks that are already selected are not considered.
\begin{algorithm}[h]
\scriptsize
\caption{m-overlapping($\mathcal{P}, B, q$)}
\label{alg:greedyM}
\KwData{$\mathcal{P}$: partitioning function, $B$: block, $q$: query}
$S\leftarrow \emptyset; R\leftarrow \emptyset$ \tcp*{Selected attributes; Resulting sub-blocks}
\While(\tcp*[f]{While unselected attributes remain}){$S \subset q.A$}{
  $B' \leftarrow \mbox{argmax}_{B'\in\mathcal{P}(B)\setminus R} \sum_{a\in B'.A \cap q.A \setminus S} \frac{c_e(B') \cdot s(a)}{s(B')}$
  $S \leftarrow S \cup B'.A$\tcp*{Extend the selected attributes}
  $R\leftarrow R \cup B'$\tcp*{Extend the selected sub-blocks}
}
\Return R \tcp*{Final set of sub-blocks covering the query attributes}
\end{algorithm} 

The query I/O cost for a block is then given by:
\begin{equation}
L(\mathcal{P}, B) = \sum_{q\in Q} \mathbf{1}(Q.T \cap B.T \neq \emptyset) \cdot \sum_{B'\in m(\mathcal{P}, B, q)} s(B')
\end{equation}

\noindent The following are unused, but may be needed for the partitioning
algorithm.

Let $f: \mathbb{R}\times A \rightarrow \mathbb{N}$ be a function that maps a
attribute-timestamp pair $(a, t)$ to the frequency of queries that access
attribute $a$ for time $t$. More formally:
\begin{equation}
f(a,t) = \sum_{q\in Q} \textbf{1}(a\in q.A \wedge t\in q.T)
\end{equation}
 
Let $T$ represent a time range $[T.s, T.e]$. We denote the frequency of
attribute access for the time interval $T$ for attribute $a$ as follows:
\begin{equation}
f(a, T) = \int_{T.s}^{T.e} f(a, t)\cdot dt =  \sum_{q\in Q} |q.T \cap T| 
\end{equation}



\bibliographystyle{abbrv}
\bibliography{main} 

\end{document}
