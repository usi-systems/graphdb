
\documentclass{sig-alternate}

\usepackage{alltt}

\begin{document}

\conferenceinfo{WOODSTOCK}{'97 El Paso, Texas USA}

\title{Adaptive Disk Storage for Interaction Graphs}


\numberofauthors{1} %  in this sample file, there are a *total*

\author{ }

\date{}

\maketitle
\begin{abstract}
TODO
\end{abstract}


\section{Introduction}
An \emph{interaction graph} is an append-only graph, where new edges and
vertices are added as time progresses. 
%

%
Typical interaction graphs have data associated with their vertices and edges.
Most algorithms access this data as they traverse the graph. However, not all
of the data is accessed by all the queries. Typically, there are correlations
among the attributes accessed by different queries, such as Q1 and Q5
accessing attributes a and b, and Q2, Q3, Q4 accessing attributes c and d, and
so on.
%

In an interaction graph database organized as blocks of temporal neighborlists
(as in my earlier paper), it is important to store the edge and vertex
properties locally. For  instance, if the edge properties are kept away in a
relational table, there will be almost no benefit to the locality
optimizations performed for block organization (discussed in \cite{gedik14}), as we would have to go back and forth between the disk blocks to
access the edge attributes. 
%

On the other hand, putting all the edge attributes into the disk blocks
containing the graph structure is expected to cause significant overhead when
only a few of these attributes are read. This is somewhat similar to the 
problem with row-oriented databases, where the entire row needs to be accessed
from the disk despite the query needing only a small fraction of it.
Unfortunately, there is no clear correspondence to a column-oriented database
layout for the graph databases.
%

Recall that interaction graphs are temporal. As such, the co-access
correlations for the attributes can be different for different temporal
regions. It might also  be unknown at the insertion time and may be discovered
later depending on the workload. This points to the need for adaptively
optimizing the layout (somewhat similar to the H2O paper).
%

I imagine a solution to this problem, which I name the 'rail layout'. The idea
is to start with large blocks that contain the entire data. As we learn about
the access properties for different time regions, we might start splitting
such blocks into smaller blocks that run parallel to each other, almost like
having two or more graph databases for certain time regions, each containing a
 different subset of the attributes, but with a link between them in case a
query needs to access both. Some attributes can be replicated as well. 
More details to follow about all this...

\section{Example}

\begin{alltt}\scriptsize
- Work through a concrete example of a database and a 
   couple of queries that would change the layout on disk.

- An interaction graph is an append-only graph, where new 
   edges and vertices are added as time progresses. 

- Typically, in graph dbs, if a vertex is visited during a 
   traversal, then it is likely that its neighbors will also 
    be visited

- So, we store we place highly connected neighbors in the 
    same disk block, to minimize disk I/O.

- Interaction graphs are temporal. The edges between vertices 
   change over time. 

- For interaction graphs, we need to group edges spatially 
   and temporally.

- Vertices and edges have data associated with them. 

- We want to store as much relevant data as possible in the 
   same block.

- We will store:
   - edges and vertices that are connected in the same time interval
   - edges and vertices that are highly connected (neighbors)
   - edge attributes that are likely to be accessed
   - vertex attributes that are likely to be accessed

- Example: We use an interaction graph to model connections
   between people on Twitter. Every time 

- $C^d(B) = conductance, lower is better.$
- $C^h(B) = cohesiveness, higher is better.$

\end{alltt}

$L(B) = \sqrt[2]{C^h(B) \cdot ( 1 - C^d(B))}$


\emph{In the problem you're describing, one natural idea that springs to mind is
  to reduce it back to a graph problem by the edges of the original interaction
  graph as vertices of an ``augmented interaction graph" whose vertex set
  consists of both the vertices and edges of the original graph, i.e. all
  entities that might have data associated to them. And two vertices of the
  augmented interaction graph are joined by an edge if either (a) both of them
  are vertices of the original interaction graph, and they are joined by an
  edge, or (b) one is an edge and the other is a vertex which is an endpoint of
  that edge. Pictorially, the augmented interaction graph is obtained by
  inserting a new vertex alongside each edge of the original interaction graph
  and turning the edge into a triangle with the new vertex as its third
  corner. It seems that this would work well assuming that queries which
  traverse an edge are likely to traverse its endpoints. -- rdk}

\section{Adaptation}

\section{Cost Model}

Let $Q$ be the query workload, where each query $q\in Q$ accesses a set of
attributes $q.A$ and traverses parts of the graph for the time range
$q.T=[q.t_s,q.t_e]$. We denote the set of all attributes as $A$. Given a block
$B$, we denote its time range as $B.T$, which is the union of the time ranges
of its temporal neighborlists. Let $s(a)$ denote the size of an attribute $a$.
We also use the same notation for block size, that is $s(B)$ for a block $B$. 
We use $c(B)$ to denote the number of entries in the temporal neighborlists
within block $B$, that is the number of edges in the block. 

Our goal is to create an overlapping partitioning of a block, denoted by
$\mathcal{P}(B)$. We call the partitions \emph{sub-blocks}. We have
$\bigcup_{B'\in \mathcal{P}(B)} B'.A = A$. We aim to find the function
$\mathcal{P}$ that minimizes the query I/O over $B$, while keeping the
relative storage overhead beyond a limit, say $1+\alpha$ times the original.
If we represent the query I/O as $L(\mathcal{P}(B))$ and the relative storage
overhead as $H(\mathcal{P}(B))$, our goal is to find:
\begin{equation}
\mathcal{P} \leftarrow \mbox{argmin}_{\{\mathcal{P}: H(\mathcal{P}(B)) < \alpha\}} L(\mathcal{P}(B))
\end{equation}

The storage overhead can be formalized as:
\begin{equation}
H(\mathcal{P}(B)) = |\mathcal{P}(B)|\cdot\left(1-\frac{\sum_{a\in A} s(a)}{s(B)}\right) 
\end{equation}

The Query I/O can be formalized as:\\
TODO for Robert.\\
We need to use the workload characteristics for the blocks time range as
well as $\mathcal{P}(B)$ to formalize the query I/O.\\


\bibliographystyle{abbrv}
\bibliography{main} 

\end{document}
