
\documentclass{sig-alternate}

\usepackage{alltt}
\usepackage{enumerate}

\begin{document}

\conferenceinfo{WOODSTOCK}{'97 El Paso, Texas USA}

\title{Adaptive Disk Storage for Interaction Graphs}


\numberofauthors{1} %  in this sample file, there are a *total*

\author{ }

\date{}

\maketitle
\begin{abstract}
TODO
\end{abstract}


\section{Introduction}
An \emph{interaction graph} is an append-only graph, where new edges and
vertices are added as time progresses. 
%

%
Typical interaction graphs have data associated with their vertices and edges.
Most algorithms access this data as they traverse the graph. However, not all
of the data is accessed by all the queries. Typically, there are correlations
among the attributes accessed by different queries, such as Q1 and Q5
accessing attributes a and b, and Q2, Q3, Q4 accessing attributes c and d, and
so on.
%

In an interaction graph database organized as blocks of temporal neighborlists
(as in my earlier paper), it is important to store the edge and vertex
properties locally. For  instance, if the edge properties are kept away in a
relational table, there will be almost no benefit to the locality
optimizations performed for block organization (discussed in \cite{gedik14}), as we would have to go back and forth between the disk blocks to
access the edge attributes. 
%

On the other hand, putting all the edge attributes into the disk blocks
containing the graph structure is expected to cause significant overhead when
only a few of these attributes are read. This is somewhat similar to the 
problem with row-oriented databases, where the entire row needs to be accessed
from the disk despite the query needing only a small fraction of it.
Unfortunately, there is no clear correspondence to a column-oriented database
layout for the graph databases.
%

Recall that interaction graphs are temporal. As such, the co-access
correlations for the attributes can be different for different temporal
regions. It might also  be unknown at the insertion time and may be discovered
later depending on the workload. This points to the need for adaptively
optimizing the layout (somewhat similar to the H2O paper).
%

I imagine a solution to this problem, which I name the 'rail layout'. The idea
is to start with large blocks that contain the entire data. As we learn about
the access properties for different time regions, we might start splitting
such blocks into smaller blocks that run parallel to each other, almost like
having two or more graph databases for certain time regions, each containing a
 different subset of the attributes, but with a link between them in case a
query needs to access both. Some attributes can be replicated as well. 
More details to follow about all this...

\section{Example}

\begin{alltt}\scriptsize
- Work through a concrete example of a database and a 
   couple of queries that would change the layout on disk.
\end{alltt}




\section{Adaptation}

\begin{itemize}

\item First, let's define a measure for the similarity between two
partitions. I think this can be the sum of the size of attributes that
they do not have in common. So, if we have partitions: abc, abd, and
abe, then abc-abd will be more similar than abc-abe, if size(d) <
size(e).

\item The first partitioning is based on what queries we have seen.
Every query gets its own sub-block. This is the "ideal" situation,
because the I/O cost would be minimized for every query that we would
have seen.

\item We calculate the storage cost for the partitioning.

\item If the storage cost is above some threshold, then we do a pairwise
comparison of all sub-blocks to determine which two are the most
similar, using (0).

\item We combine the two most-similar sub-blocks, and repeat the process
starting at (2) until we are satisfied with the storage cost.

\item The result is our partitioned block.

\end{itemize}

\section{Cost Model}

Let $Q$ be the query workload, where each query $q\in Q$ accesses a set of
attributes $q.A$ and traverses parts of the graph for the time range
$q.T=[q.t_s,q.t_e]$. We denote the set of all attributes as $A$. Given a block
$B$, we denote its time range as $B.T$, which is the union of the time ranges
of its temporal neighborlists. Let $s(a)$ denote the size of an attribute $a$.
We use $c_n(B)$ to denote the number of temporal neighborlists within block
$B$ and $c_e(B)$ to denote the total number of edges in the temporal
neighborlists within the block. We overload the notation for block size and
use $s(B)$ to denote the size of a block $B$. We have: 
\begin{equation}
s(B) = c_e(B) \cdot \left(16 + \sum_{a\in A} s(a)\right) + c_n(B) \cdot 12  
\end{equation}
Here, $16$ corresponds to the cost of storing the edge id and the timestamp,
and $qw$ corresponds to the cost of storing the head vertex ($8$ bytes) plus
the number of entries ($4$ bytes) for a temporal neighborlist. 

Our goal is to create an overlapping partitioning of a block, denoted by
$\mathcal{P}(B)$. We call the partitions \emph{sub-blocks}. We have
$\bigcup_{B'\in \mathcal{P}(B)} B'.A = A$. We aim to find the function
$\mathcal{P}$ that minimizes the query I/O over $B$, while keeping the
relative storage overhead beyond a limit, say $1+\alpha$ times the original.
If we represent the query I/O as $L(\mathcal{P}(B))$ and the relative storage
overhead as $H(\mathcal{P}(B))$, our goal is to find:
\begin{equation}
\mathcal{P} \leftarrow \mbox{argmin}_{\{\mathcal{P}: H(\mathcal{P}(B)) < \alpha\}} L(\mathcal{P}(B))
\end{equation}

The storage overhead can be formalized as:
\begin{equation}
H(\mathcal{P}(B)) = |\mathcal{P}(B)|\cdot\left(1-\frac{c_e(B)\cdot \sum_{a\in A} s(a)}{s(B)}\right) 
\end{equation}

The Query I/O can be formalized as:\\
TODO for Robert.\\
We need to use the workload characteristics for the blocks time range as
well as $\mathcal{P}(B)$ to formalize the query I/O.\\

\subsection{Query I/0}

We are also given a function $f$ that maps a query to the frequency the
query is executed during a particular time range $[t_s,t_e]$:
$$
f:  \text{query} \rightarrow \text{time} * \text{time} \rightarrow \text{frequency}
$$

\noindent 
We are also given a function $m$ that maps a query to the set of sub-blocks that are
accessed to satisfy that query during a particular time range. 
$$
m:  \text{query} \rightarrow \text{time} * \text{time} \rightarrow \text{block set}
$$

\noindent Define:
$$
\bigcup_{B'\in m(q, q.t_s, q.t_e)} B'.A = M
$$

\noindent 
Generalizing, I think we want an equation like the following:
$$
L(\mathcal{P}(B)) = \sum_{q\in Q} \left( f(q, q.t_s, q.t_e) \cdot \left( \sum_{a \in M}
  s(a) + \left\vert{m(q, q.t_s, q.t_e)}\right\vert \cdot c(B)\right) \right)
$$



\bibliographystyle{abbrv}
\bibliography{main} 

\end{document}
