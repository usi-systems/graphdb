
\documentclass[twocolumn]{svjour3}          % twocolumn
%
\smartqed  % flush right qed marks, e.g. at end of proof
%

\usepackage{alltt}
\usepackage{enumerate}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{xargs}
\usepackage{amssymb}
\usepackage{amsmath}

\renewcommand{\CommentSty}[1]{\textnormal{#1}}
\DontPrintSemicolon
\SetKwComment{tcp}{$\triangleright$ }{}
\SetVlineSkip{0cm}
\SetAlgoSkip{}

\let\oldemptyset\emptyset
\let\emptyset\varnothing


% Insert the name of "your journal" with
\journalname{VLDBJ}
%
\begin{document}

\title{Adaptive Disk Storage for Interaction Graphs}

%\titlerunning{Short form of title}        % if too long for running head

\author{Bu\u{g}ra Gedik         \and
        Robert Soul\'{e}%etc.
}

%\authorrunning{Short form of author list} % if too long for running head

\institute{B. Gedik \at            
              \email{bgedik@cs.bilkent.edu.tr}           %  \\
           \and
           R. Soul\'{e} \at
               \email{robert.soule@usi.ch}     
}

\date{Received: date / Accepted: date}
% The correct dates will be entered by the editor


\maketitle

\begin{abstract}
Insert your abstract here. Include keywords, PACS and mathematical
subject classification numbers as needed.
\keywords{First keyword \and Second keyword \and More}
% \PACS{PACS code1 \and PACS code2 \and more}
% \subclass{MSC code1 \and MSC code2 \and more}
\end{abstract}



\section{Introduction}
An \emph{interaction graph} is an append-only graph, where new edges and
vertices are added as time progresses. 
%

% 
Typical interaction graphs have data associated with their vertices and edges.
Most algorithms access this data as they traverse the graph. However, not all
of the data is accessed by all the queries. Typically, there are correlations
among the attributes accessed by different queries, such as Q1 and Q5
accessing attributes a and b, and Q2, Q3, Q4 accessing attributes c and d, and
so on.
%

In an interaction graph database organized as blocks of temporal neighborlists
(as in Gedik et al.~\cite{gedik14}), it is important to store the edge and vertex
properties locally. For  instance, if the edge properties are kept away in a
relational table, there will be almost no benefit to the locality
optimizations performed for block organization (discussed in Gedik et al.~\cite{gedik14}, as we would have to go back and forth between the disk blocks to
access the edge attributes. 
%

On the other hand, putting all the edge attributes into the disk blocks
containing the graph structure is expected to cause significant overhead when
only a few of these attributes are read. This is somewhat similar to the 
problem with row-oriented databases, where the entire row needs to be accessed
from the disk despite the query needing only a small fraction of it.
Unfortunately, there is no clear correspondence to a column-oriented database
layout for the graph databases.
%

Recall that interaction graphs are temporal. As such, the co-access
correlations for the attributes can be different for different temporal
regions. It might also  be unknown at the insertion time and may be discovered
later depending on the workload. This points to the need for adaptively
optimizing the layout (somewhat similar to the H2O paper).
%

I imagine a solution to this problem, which I name the 'rail layout'. The idea
is to start with large blocks that contain the entire data. As we learn about
the access properties for different time regions, we might start splitting
such blocks into smaller blocks that run parallel to each other, almost like
having two or more graph databases for certain time regions, each containing a
 different subset of the attributes, but with a link between them in case a
query needs to access both. Some attributes can be replicated as well. 
More details to follow about all this...

\section{Example}

\begin{alltt}\scriptsize
- Work through a concrete example of a database and a 
   couple of queries that would change the layout on disk.
\end{alltt}




\section{Cost Model}
Let $Q$ be the query workload, where each query $q\in Q$ accesses a set of
attributes $q.A$ and traverses parts of the graph for the time range
$q.T=[q.t_s,q.t_e]$. We denote the set of all attributes as $A$. Given a block
$B$, we denote its time range as $B.T$, which is the union of the time ranges
of its temporal neighborlists. Let $s(a)$ denote the size of an attribute $a$.
We use $c_n(B)$ to denote the number of temporal neighborlists within block
$B$ and $c_e(B)$ to denote the total number of edges in the temporal
neighborlists within the block. We overload the notation for block size and
use $s(B)$ to denote the size of a block $B$. We have: 
\begin{equation}
s(B) = c_e(B) \cdot \left(16 + \sum_{a\in A} s(a)\right) + c_n(B) \cdot 12  
\end{equation}
Here, $16$ corresponds to the cost of storing the edge id and the timestamp,
and $qw$ corresponds to the cost of storing the head vertex ($8$ bytes) plus
the number of entries ($4$ bytes) for a temporal neighborlist. 

Our goal is to create an overlapping partitioning of a block, denoted by
$\mathcal{P}(B)$. We call the partitions \emph{sub-blocks}. We have
$\bigcup_{B'\in \mathcal{P}(B)} B'.A = A$. We aim to find the function
$\mathcal{P}$ that minimizes the query I/O over $B$, while keeping the
relative storage overhead beyond a limit, say $1+\alpha$ times the original.
If we represent the query I/O as $L(\mathcal{P}, B)$ and the relative storage
overhead as $H(\mathcal{P}, B)$, our goal is to find:
\begin{equation}
\mathcal{P} \leftarrow \mbox{argmin}_{\{\mathcal{P}: H(\mathcal{P}(B)) < \alpha\}} L(\mathcal{P},B)
\end{equation}

The storage overhead can be formalized as:
\begin{equation}
H(\mathcal{P}, B) = |\mathcal{P}(B)|\cdot\left(1-\frac{c_e(B)\cdot \sum_{a\in A} s(a)}{s(B)}\right) 
\end{equation}

\subsection{Query I/O}

Let $m$ be a function that maps a query $q$ to the set of sub-blocks that are
accessed to satisfy it for a relevant block $B$ under a given partitioning
$\mathcal{P}$. For the case of non-overlapping attributes, we have:
\begin{equation}
m(\mathcal{P}, B, q) = \{B': B'\in \mathcal{P}(B) \wedge q.A \cap B'.A \ne \emptyset\}  
\end{equation}

For the case of overlapping attributes, we use a simple heuristic to define the
set of sub-blocks to be used for answering the query.
Algorithm~\ref{alg:greedyM} captures it. The idea is to select sub-blocks that
bring the highest relative marginal gain, which is defined as the size of
attribute data that contributes to the query result, relative to the sub-block
size. While computing the marginal gain, attributes that are covered by
sub-blocks that are already selected are not considered.
%
\begin{algorithm}[h]
\scriptsize
\caption{m-overlapping($\mathcal{P}, B, q$)}
\label{alg:greedyM}
\KwData{$\mathcal{P}$: partitioning function, $B$: block, $q$: query}
$S\leftarrow \emptyset; R\leftarrow \emptyset$ \tcp*{Selected attributes; Resulting sub-blocks}
\While(\tcp*[f]{While unselected attributes remain}){$S \subset q.A$}{
  $B' \leftarrow \mbox{argmax}_{B'\in\mathcal{P}(B)\setminus R} \sum_{a\in B'.A \cap q.A \setminus S} \frac{c_e(B') \cdot s(a)}{s(B')}$
  $S \leftarrow S \cup B'.A$\tcp*{Extend the selected attributes}
  $R\leftarrow R \cup B'$\tcp*{Extend the selected sub-blocks}
}
\Return R \tcp*{Final set of sub-blocks covering the query attributes}
\end{algorithm} 
%
The query I/O cost for a block is then given by:
\begin{equation}
L(\mathcal{P}, B) = \sum_{q\in Q} \mathbf{1}(Q.T \cap B.T \neq \emptyset) \cdot \sum_{B'\in m(\mathcal{P}, B, q)} s(B')
\end{equation}

\noindent The following are unused, but may be needed for the partitioning
algorithm.

Let $f: \mathbb{R}\times A \rightarrow \mathbb{N}$ be a function that maps a
attribute-timestamp pair $(a, t)$ to the frequency of queries that access
attribute $a$ for time $t$. More formally:
\begin{equation}
f(a,t) = \sum_{q\in Q} \textbf{1}(a\in q.A \wedge t\in q.T)
\end{equation}
 
Let $T$ represent a time range $[T.s, T.e]$. We denote the frequency of
attribute access for the time interval $T$ for attribute $a$ as follows:
\begin{equation}
f(a, T) = \int_{T.s}^{T.e} f(a, t)\cdot dt =  \sum_{q\in Q} |q.T \cap T| 
\end{equation}



\section{Adaptation}

\subsection{Non-Overlapping Attributes}

%\linprog{f(x)}{g_i(x) \leq 0}{\eqnlimit[1]{i}[n]}{h_j(x) = 0}{\eqnlimit[1]{j}[m]}{eq:linprog1}


 \begin{alignat*}{2}
    \text{minimize: }   & L(\mathcal{P}, B)   \\
    \text{subject to: } & H(\mathcal{P}, B) \leq \text{Max Storage}
  \end{alignat*}

\subsection{Overlapping Attributes}

\begin{itemize}

\item First, let's define a measure for the similarity between two
partitions. I think this can be the sum of the size of attributes that
they do not have in common. So, if we have partitions: abc, abd, and
abe, then abc-abd will be more similar than abc-abe, if size(d) <
size(e).

\item The first partitioning is based on what queries we have seen.
Every query gets its own sub-block. This is the "ideal" situation,
because the I/O cost would be minimized for every query that we would
have seen.

\item We calculate the storage cost for the partitioning.

\item If the storage cost is above some threshold, then we do a pairwise
comparison of all sub-blocks to determine which two are the most
similar, using (0).

\item We combine the two most-similar sub-blocks, and repeat the process
starting at (2) until we are satisfied with the storage cost.

\item The result is our partitioned block.

\end{itemize}


\begin{algorithm}[h]
\scriptsize
\caption{Algorithm for partitioning blocks into sub-blocks with overlapping attributes.}
\label{alg:overlappingP}
\KwData{$B$: block, $Q$: set of queries}
$\mathcal{P}(B) \leftarrow \emptyset;$  \tcp*{Sub-blocks;}

\For(\tcp*[f]{Every query gets its own sub-block}){$q \in Q$}{ 
 $\mathcal{P}(B) = \mathcal{P}(B) \cup q.A$ 
}

\While(\tcp*[f]{While storage exceeds threshold}){$H(\mathcal{P}, B) \geq \text{max}$}{
    $min = \infty$ \; 
    $x = \emptyset$ \;
    $y = \emptyset$ \;
    \For{$b_1 \in \mathcal{P}(B)$}{ 
        \For{$b_2 \in \mathcal{P}(B)$}{ 
            $\delta = (b_1 \cup b_2) - (b_1 \cap b_2)$ \; 
           \If{$min < \delta $}{
                 $\text{x} \leftarrow  b_1$\;
                 $\text{y} \leftarrow b_2$\;
                 $min \leftarrow \delta$
            } 
        }
    }
$ \mathcal{P}(B)  \leftarrow \mathcal{P}(B) - x - y$ \;
 $\mathcal{P}(B)  \leftarrow \mathcal{P}(B)  + x \cup y$ \;
}
\Return $ \mathcal{P}(B)$  \tcp*{Final set of sub-blocks}
\end{algorithm} 



%\begin{acknowledgements}
%If you'd like to thank anyone, place your comments here
%and remove the percent signs.
%\end{acknowledgements}

\bibliographystyle{spmpsci}
\bibliography{main} 



\end{document}
% end of file template.tex

