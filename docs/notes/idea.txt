Typical interaction graphs have data associated with their vertices and edges.
Most algorithms access this data as they traverse the graph. However, not all
of the data is accessed by all the queries. Typically, there exist joint
access correlations among the attributes read by different queries, such as Q1
and Q5 accessing attributes a and b, and Q2, Q3, Q4 accessing attributes c and
d, and so on.

In an interaction graph database organized as blocks of temporal neighborlists
(as in my earlier paper), it is important to store the edge and vertex
properties locally. For  instance, if the edge properties are kept away in a
relational table, there will be almost no benefit to the locality
optimizations performed for block organization (discussed in my earlier
paper), as we would have to go back and forth between the disk blocks to
access the edge attributes. 

On the other hand, putting all the edge attributes into the disk blocks
containing the graph structure is expected to cause significant I/O overhead
when only a few of these attributes are read. This is somewhat similar to the 
problem with row-oriented databases, where the entire row needs to be accessed
from the disk despite the query needing only a small fraction of it.
Unfortunately, there is no clear correspondence to a column-oriented database
layout for the graph databases.

Recall that interaction graphs are temporal. As such, the co-access
correlations for the attributes can be different for different temporal
regions. It might also  be unknown at the insertion time and may be discovered
later depending on the workload. This points to the need for adaptively
optimizing the layout (somewhat similar to the H2O paper).

I imagine a solution to this problem, which I name the 'rail layout'. The idea
is to start with large blocks that contain the entire attribute data. As we
learn about the access properties for different time regions, we might start
splitting such blocks into smaller blocks that run parallel to each other,
almost like having two or more graph databases for certain time regions, each
containing a  different subset of the attributes, but with a link between them
in case a query needs to access both. Some attributes can be replicated as
well.  

Initially the graph database is organized as a set of blocks. Each block
contains a set of temporal neighborlists. Both the vertices and the edges in a
block initially keeps around all the attribute data. We use a temporal index
to keep access frequencies of different attributes for queries running over
different temporal regions. Let us assume that after some time, our frequency
index tells us that within the time interval [t_1,t_2], most of the queries
are accessing the edge attributes a and b. The next time we get one such
query, we divide the blocks accessed by it into two: one block that contains
attributes a and b, and another that contains the remaining attributes. There
will be a link between the two as well, in case a query requires attributes
from both blocks. Let us call the new blocks formed after a split, sub-blocks.
We replace the original block with these sub-blocks. For efficiency purposes
we need to make sure that connections that go out from one sub-block to
another are adjusted so that the attributes stored on the connected sub-blocks
are similar. That is, if a block X has two sub-blocks, X_1 keeping attributes
a and b, and X_2 keeping attributes c and d; and if  a block Y has two
sub-blocks, Y_1 keeping attributes a and b, and Y_2 keeping attributes c and
d; then if there is an original connection between X and Y, then there should
be a connection between X_1 and Y_1 as well as X_2 and Y_2. While other
alternatives are possible as well, this one is the most efficient in terms of
minimizing I/O.